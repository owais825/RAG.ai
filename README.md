# RAG.ai
ğŸ“„ RAG with LangChain and Gemini - AI PDF Assistant

A Streamlit-based AI assistant that allows users to upload a PDF document and ask context-based questions about its content. This app uses LangChain, FAISS, HuggingFace embeddings, and Google Gemini to retrieve and generate accurate answers.

ğŸš€ Features
- ğŸ“„ Upload any PDF file and extract text
- ğŸ” Split the PDF into manageable chunks using LangChain
- ğŸ§  Store and search chunks using FAISS Vector DB
- ğŸ¤– Generate answers with Google Gemini (Gemini 2.0 Flash)
- ğŸ’¬ Simple and intuitive Streamlit interface

ğŸ› ï¸ Tech Stack
- Streamlit
- LangChain
- FAISS
- HuggingFace Transformers (all-MiniLM-L6-v2)
- Google Generative AI (Gemini)

ğŸ” Setup and Installation

1. Clone the repository
   git clone https://github.com/owais825/RAG.ai.git
   
   cd ai-doc-assistant

2. Create and activate a virtual environment
   python -m venv venv
   source venv/bin/activate  (on Windows: venv\Scripts\activate)

3. Install dependencies
   pip install -r requirements.txt

4. Set up environment variables
   Create a .env file in the root directory and add your Gemini API key:
   GOOGLE-API-KEY=your_gemini_api_key

â–¶ï¸ Run the App
   streamlit run app.py

   Open the app in your browser, upload a PDF, and start asking questions!

ğŸ“‚ Project Structure
.
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ .env                # Environment variables (not committed)
â”œâ”€â”€ README.md           # Project documentation
â”œâ”€â”€ requirements.txt    # Python dependencies

âš ï¸ Limitations
- Only supports text-based PDFs
- Works best with concise or well-structured documents
- Gemini output may slightly vary due to the generative nature of LLMs

ğŸ™Œ Acknowledgements
- Google Generative AI
- HuggingFace (MiniLM embeddings)
- LangChain for RAG pipeline
- Streamlit for rapid UI development

ğŸ“¬ Contact
For questions or suggestions, contact: owais825@gmail.com